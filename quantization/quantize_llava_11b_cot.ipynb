{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers bitsandbytes accelerate optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import bitsandbytes as bnb\n",
    "import numpy\n",
    "import scipy\n",
    "import platform\n",
    "\n",
    "# Print versions\n",
    "print(\"Library Versions:\")\n",
    "print(f\"Python: {platform.python_version()}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"BitsAndBytes: {bnb.__version__}\")\n",
    "print(f\"NumPy: {numpy.__version__}\")\n",
    "print(f\"SciPy: {scipy.__version__}\")\n",
    "\n",
    "# # Save versions to a file\n",
    "# with open(\"library_versions.txt\", \"w\") as f:\n",
    "#     f.write(\"Library Versions:\\n\")\n",
    "#     f.write(f\"Python: {platform.python_version()}\\n\")\n",
    "#     f.write(f\"PyTorch: {torch.__version__}\\n\")\n",
    "#     f.write(f\"Transformers: {transformers.__version__}\\n\")\n",
    "#     f.write(f\"BitsAndBytes: {bnb.__version__}\\n\")\n",
    "#     f.write(f\"NumPy: {numpy.__version__}\\n\")\n",
    "#     f.write(f\"SciPy: {scipy.__version__}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Xkev/Llama-3.2V-11B-cot\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download regular model and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "model_id = \"Xkev/Llama-3.2V-11B-cot\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "print(f\"Saving the model: {model_id}\")\n",
    "model.save_pretrained(\"llava-11b-cot\")\n",
    "print(f\"Saving the processor: {model_id}\")\n",
    "processor.save_pretrained(\"llava-11b-cot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Int8 Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses the `bitsandbytes` library integrated into transformers to perform the int8 quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "model_id = \"Xkev/Llama-3.2V-11B-cot\"\n",
    "\n",
    "# Load processor and model\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_8bit=True,  # Enable INT8 quantization\n",
    "    device_map=\"auto\",  # Automatically map layers to GPU\n",
    ")\n",
    "print(f\"Finished creating the int8 quantization...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"llava-11b-cot-int8\"\n",
    "\n",
    "print(\"Saving int8 quantized version...\")\n",
    "model.save_pretrained(save_path)\n",
    "print(\"Saving the processor...\")\n",
    "processor.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Int4 Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the following for more information: https://huggingface.co/blog/4bit-transformers-bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4int_quant_type='nf4', \n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_nf4 = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config = nf4_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and processor\n",
    "save_path = \"llava-11b-cot-nf4\"\n",
    "model.save_pretrained(save_path)\n",
    "processor.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int4 quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_int4 = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"llava-11b-cot-int4\"\n",
    "model_int4.save_pretrained(save_path)\n",
    "processor.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
